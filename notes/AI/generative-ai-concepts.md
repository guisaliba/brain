# subject: [[generative-ai]]
# topics: #LLMs #ai 

**Tokenization:** Tokenization is the process of breaking down an input into smaller chunks, such as words or subwords, to enable the model to process and generate language more effectively. Different tokenization strategies can be employed, such as ignoring punctuation or breaking long words into multiple tokens.

**Context length:** he context length represents the number of tokens the model considers when generating each subsequent token. A longer context length can result in more precise output, as the model has more information to draw upon when making predictions.

**Context window:**

**Attention mechanism:** he attention mechanism assigns a score to each token based on its relevance to the current sentence, position in the sequence, and similarity to adjacent tokens. These scores are used to guide the model's prediction of the next token.

**Output generation:** The model selects the most likely candidate token based on its probability distribution, using both the attention weights and the model's parameters. This process continues until the desired output is generated.

By combining these components, LLMs can process and generate language that is both coherent and contextually appropriate.